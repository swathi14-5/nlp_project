{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS_NER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tw-ll7pwF4QU"
      },
      "source": [
        "### Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7-piwdovGck",
        "outputId": "22770ea4-a6f1-4dc0-fac9-f5ad71750e67"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "nltk.download(\"words\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-YbNiUcF-8n"
      },
      "source": [
        "### Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyrY5HrhtznB",
        "outputId": "c6f5c658-d35a-4527-c48f-19d760eb90e4"
      },
      "source": [
        "txt = \"personally led a rescue operation to free Indonesian crew members taken hostage by pirates in June 2020\"\n",
        "tokenized = sent_tokenize(txt)\n",
        "print(tokenized)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['personally led a rescue operation to free Indonesian crew members taken hostage by pirates in June 2020']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0cYLOfzGC6t"
      },
      "source": [
        "### String Tokenization And Parts of Speech Tagging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WlrhAWcvQH5",
        "outputId": "fbc845c9-131a-46f7-bc7c-604b80810216"
      },
      "source": [
        "# Passing sentence by sentence in the for loop\n",
        "for token in tokenized:\n",
        "    # Tokenizing the sentence\n",
        "    wordsList = nltk.word_tokenize(token)\n",
        "    # Removing stop words like is, on , and, ...\n",
        "    wordsList = [w for w in wordsList if not w in stop_words]\n",
        "    # Task1 Call the method that does Parts of Speech Tagging from nltk library and the pass the wordsList\n",
        "    # tagged = ...\n",
        "    # Your code here\n",
        "    tagged = nltk.pos_tag(wordsList)\n",
        "    \n",
        "    print(tagged)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('personally', 'RB'), ('led', 'VBN'), ('rescue', 'NN'), ('operation', 'NN'), ('free', 'JJ'), ('Indonesian', 'NNP'), ('crew', 'NN'), ('members', 'NNS'), ('taken', 'VBN'), ('hostage', 'NN'), ('pirates', 'VBZ'), ('June', 'NNP'), ('2020', 'CD')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reckuQn2tEwd"
      },
      "source": [
        "Representations:\n",
        "\n",
        "CC \tcoordinating conjunction\n",
        "CD \tcardinal digit\n",
        "DT \tdeterminer\n",
        "EX \texistential there\n",
        "FW \tforeign word\n",
        "IN \tpreposition/subordinating conjunction\n",
        "JJ \tThis NLTK POS Tag is an adjective (large)\n",
        "JJR \tadjective, comparative (larger)\n",
        "JJS \tadjective, superlative (largest)\n",
        "LS \tlist market\n",
        "MD \tmodal (could, will)\n",
        "NN \tnoun, singular (cat, tree)\n",
        "NNS \tnoun plural (desks)\n",
        "NNP \tproper noun, singular (sarah)\n",
        "NNPS \tproper noun, plural (indians or americans)\n",
        "PDT \tpredeterminer (all, both, half)\n",
        "POS \tpossessive ending (parent\\ 's)\n",
        "PRP \tpersonal pronoun (hers, herself, him,himself)\n",
        "PRP$ \tpossessive pronoun (her, his, mine, my, our )\n",
        "RB \tadverb (occasionally, swiftly)\n",
        "RBR \tadverb, comparative (greater)\n",
        "RBS \tadverb, superlative (biggest)\n",
        "RP \tparticle (about)\n",
        "TO \tinfinite marker (to)\n",
        "UH \tinterjection (goodbye)\n",
        "VB \tverb (ask)\n",
        "VBG \tverb gerund (judging)\n",
        "VBD \tverb past tense (pleaded)\n",
        "VBN \tverb past participle (reunified)\n",
        "VBP \tverb, present tense not 3rd person singular(wrap)\n",
        "VBZ \tverb, present tense with 3rd person singular (bases)\n",
        "WDT \twh-determiner (that, what)\n",
        "WP \twh- pronoun (who)\n",
        "WRB \twh- adverb (how) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA5YD9sN5F43"
      },
      "source": [
        "\n",
        "### Named Entity Recognition\n",
        "\n",
        "**Named Entity Recognition** (NER) is a standard NLP problem which involves spotting named entities (people, places, organizations etc.) from a chunk of text, and classifying them into a predefined set of categories.\n",
        "\n",
        "Refer: https://www.nltk.org/book/ch07.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNJBylHZ5KUe",
        "outputId": "d187bd42-721d-4fbf-923d-a8496cb7dd7f"
      },
      "source": [
        "# Sample sentence\n",
        "\n",
        "sentence = \"personally led a rescue operation to free Indonesian crew members taken hostage by pirates in June 2021\"\n",
        "\n",
        "# Sending sentence by sentence inside the loop\n",
        "for sent in nltk.sent_tokenize(sentence):\n",
        "    # We will form chunks based on the tokenized words and pos tagging\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "        # The chunks will hold a attribute label if it has identified that the chunk is a Named Entity\n",
        "        if hasattr(chunk, 'label'):\n",
        "            print(chunk.label(), ' '.join(c[0] for c in chunk))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPE Indonesian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP_ikUtEtEwh"
      },
      "source": [
        "***NE Type \tExamples***\n",
        "ORGANIZATION \tGeorgia-Pacific Corp., \n",
        "PERSON \tEddy Bonte, President Obama\n",
        "LOCATION \tMurray River, Mount Everest\n",
        "DATE \tJune, 2008-06-29\n",
        "TIME \ttwo fifty a m, 1:30 p.m.\n",
        "MONEY \t175 million Canadian Dollars, \n",
        "PERCENT \ttwenty pct, 18.75 %\n",
        "FACILITY \tWashington Monument, Stonehenge\n",
        "GPE \tSouth East Asia, Midlothian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFRWYNpZ5ZGN"
      },
      "source": [
        "# Task 2 \n",
        "# List a few named entities that you can think of.\n",
        "# Check if those entities are being caputered by nltk library by passing a sentence containing that named entitiy in the above code."
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZoJsJGVtEwj",
        "outputId": "31096a29-16ce-4848-d064-85f4600105b5"
      },
      "source": [
        "# Example for other Parsers\n",
        "sentences = \"Donald John Trump is an American media personality and businessman who served as the 45th president of the United States from 2017 to 2021. Born and raised in Queens, New York City, Trump attended Fordham University and the University of Pennsylvania, graduating with a bachelor's degree in 1968.\"\n",
        "sentence1 = \"Barack Hussein Obama Jr. is an American politician and attorney who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, Obama was the first African-American president of the United States.\"\n",
        "\n",
        "print(\"\\nSentence 1:\\n\")\n",
        "# Sending sentence by sentence inside the loop\n",
        "for sent in nltk.sent_tokenize(sentences):\n",
        "    # We will form chunks based on the tokenized words and pos tagging\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "        # The chunks will hold a attribute label if it has identified that the chunk is a Named Entity\n",
        "        if hasattr(chunk, 'label'):\n",
        "            print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
        "\n",
        "print(\"\\nSentence 2:\\n\")\n",
        "\n",
        " # Sending sentence by sentence inside the loop\n",
        "for sent in nltk.sent_tokenize(sentence1):\n",
        "    # We will form chunks based on the tokenized words and pos tagging\n",
        "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
        "        # The chunks will hold a attribute label if it has identified that the chunk is a Named Entity\n",
        "        if hasattr(chunk, 'label'):\n",
        "            print(chunk.label(), ' '.join(c[0] for c in chunk))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Sentence 1:\n",
            "\n",
            "PERSON Donald\n",
            "PERSON John Trump\n",
            "GPE American\n",
            "GPE United States\n",
            "GPE Born\n",
            "GPE Queens\n",
            "GPE New York City\n",
            "PERSON Trump\n",
            "ORGANIZATION Fordham University\n",
            "ORGANIZATION University\n",
            "GPE Pennsylvania\n",
            "\n",
            "Sentence 2:\n",
            "\n",
            "PERSON Barack\n",
            "PERSON Hussein Obama\n",
            "GPE American\n",
            "GPE United States\n",
            "ORGANIZATION Democratic Party\n",
            "PERSON Obama\n",
            "GPE United States\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOBb3wGWtEwj",
        "outputId": "686f06a0-f28e-41e2-ce55-bcd5b337ebc7"
      },
      "source": [
        "from nltk.parse.stanford import StanfordDependencyParser\n",
        "\n",
        "# Download the parser from https://nlp.stanford.edu/software/stanford-parser-4.2.0.zip\n",
        "# Extract the folder and link stanford-parser.jar\n",
        "path_jar = \"./stanford-parser.jar\"\n",
        "# Link stanford-parser-4.2.0-models.jar\n",
        "path_models_jar = \"./stanford-parser-4.2.0-models.jar\"\n",
        "\n",
        "dep_parser = StanfordDependencyParser(path_to_jar = path_jar, path_to_models_jar = path_models_jar)\n",
        "result = dep_parser.raw_parse(\"I saw an elephant in my sleep\")\n",
        "dependency = result.__next__()\n",
        "\n",
        "#Print the results of the parser\n",
        "print(list(dependency.triples()))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(('saw', 'VBD'), 'nsubj', ('I', 'PRP')), (('saw', 'VBD'), 'obj', ('elephant', 'NN')), (('elephant', 'NN'), 'det', ('an', 'DT')), (('saw', 'VBD'), 'obl', ('sleep', 'NN')), (('sleep', 'NN'), 'case', ('in', 'IN')), (('sleep', 'NN'), 'nmod:poss', ('my', 'PRP$'))]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36d92P47HKO0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}